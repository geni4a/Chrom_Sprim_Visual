{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02181fe5-f283-4318-8126-0e9e9a8ec134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pybedtools\n",
    "from itertools import combinations\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eeb91ba-93d5-43be-b1f1-5bb0a1b7ff30",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/eugeniaampofo/Downloads/Downloads/Sprime_files/Jupyter_Vis/Jupyter/Human_GRCH37_Chromosome_bases_lenghts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#document with chromosome lengths in bases for human GRCh37. Found on this website: https://www.ncbi.nlm.nih.gov/grc/human/data?asm=GRCh37#:~:text=Chromosome%2C%201%2C%202%2C%203%2C%204%2C%20Total%20length,191%2C154%2C276%2C%20GenBank%20accession%2C%20CM000663.1%2C%20CM000664.1%2C%20CM000665.1%2C%20CM000666.1%2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chrom_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/eugeniaampofo/Downloads/Downloads/Sprime_files/Jupyter_Vis/Jupyter/Human_GRCH37_Chromosome_bases_lenghts.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m chrom_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChromosome\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCHROM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal length (bp)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_len_bp\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# focus only on autosomal chromosomes\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/eugeniaampofo/Downloads/Downloads/Sprime_files/Jupyter_Vis/Jupyter/Human_GRCH37_Chromosome_bases_lenghts.csv'"
     ]
    }
   ],
   "source": [
    "#document with chromosome lengths in bases for human GRCh37. Found on this website: https://www.ncbi.nlm.nih.gov/grc/human/data?asm=GRCh37#:~:text=Chromosome%2C%201%2C%202%2C%203%2C%204%2C%20Total%20length,191%2C154%2C276%2C%20GenBank%20accession%2C%20CM000663.1%2C%20CM000664.1%2C%20CM000665.1%2C%20CM000666.1%2\n",
    "chrom_df = pd.read_csv(\"/Users/eugeniaampofo/Downloads/Downloads/Sprime_files/Jupyter_Vis/Jupyter/Human_GRCH37_Chromosome_bases_lenghts.csv\") \n",
    "chrom_df.rename(columns={'Chromosome': 'CHROM', \"Total length (bp)\": \"Total_len_bp\"}, inplace=True)\n",
    "# focus only on autosomal chromosomes\n",
    "chrom_df = chrom_df.loc[(chrom_df[\"CHROM\"] != \"X\") & (chrom_df[\"CHROM\"] != \"Y\")] \n",
    "chrom_df[\"Total_len_bp\"] = chrom_df[\"Total_len_bp\"].str.replace(',', '').astype(int)\n",
    "chrom_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633cc8d-36de-467e-800d-c181ab59d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = sum(chrom_df[\"Total_len_bp\"].to_numpy())/100   #for genome proportion calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149586f-5ba1-47cc-b312-98a0bfdf8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sprime_files = \"/Users/eugeniaampofo/Downloads/Downloads/Sprime_files/Sprime_res/\" #location of Sprime Outputs\n",
    "populations = []\n",
    "\n",
    "for file in os.listdir(sprime_files): #finding names of populations\n",
    "    if \"_sprime_results.tar.gz\" in file:\n",
    "        name = file.replace(\"_sprime_results.tar.gz\" ,\"\")\n",
    "        populations.append(name)\n",
    "populations = list(set(populations))\n",
    "populations = sorted(populations)\n",
    "populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49908e63-81cd-4eae-8429-6e19239d2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generate all pairs of populations \n",
    "population_pairs = list(combinations(populations, 2))\n",
    "population_pairs = [sorted(list(x)) for x in population_pairs]\n",
    "population_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fcc1c-dcf2-48e9-b509-707e14278e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_seg(df): #calculates segment length\n",
    "    k = df[\"END\"].to_numpy() - df[\"START\"].to_numpy()\n",
    "    return np.sum(k)\n",
    "\n",
    "def sum_seg2(df): #calculates segment length\n",
    "    lens = []\n",
    "    for s in df.START.unique():\n",
    "        e = df.loc[df[\"START\"] == s]\n",
    "        k = e[\"END\"].to_numpy() - e[\"START\"].to_numpy()\n",
    "        lens.append(k)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315cc35-3eb8-40a3-be42-19dd40690872",
   "metadata": {},
   "outputs": [],
   "source": [
    "sprime_files = \"/Users/eugeniaampofo/Downloads/Downloads/Sprime_files/Sprime_res/mendeley_data/\" #Sprime outputs\n",
    "segment_lens = {} #tracks genome coverage of introgressed segments that are also have neanderthal/denisovan info \n",
    "adjusted_lens = {} # ^ + dealing with tiling by using median/mean\n",
    "for pop in populations:\n",
    "    seg_len = 0 #track of total genome coverage for chromosome\n",
    "    seg_p = [] #track of each ch\n",
    "    for i in range(1,23):\n",
    "        for file in os.listdir(sprime_files):\n",
    "            if pop in file and f\"chr{i}.\" in file:\n",
    "                df= pd.read_csv(sprime_files + file, sep=\"\\t\") \n",
    "                df[\"CHROM\"] = df[\"CHROM\"].astype(int)\n",
    "                df[\"POS\"] = df[\"POS\"].astype(int)\n",
    "                df[\"SEGMENT\"] = df[\"SEGMENT\"].astype(int)\n",
    "                df_sorted = df.sort_values(by=[\"CHROM\", \"POS\", \"SEGMENT\"], ascending=True)\n",
    "                # print(df)\n",
    "                df = df.loc[df[\"SCORE\"] >= 150000] \n",
    "                df = df.loc[(df[\"NMATCH\"] != \"notcomp\") | (df[\"DMATCH\"] != \"notcomp\")] #filter out sites wit notcomp for both Nean and Deni\n",
    "                df_sorted = df.sort_values(by=[\"CHROM\", \"POS\", \"SEGMENT\"], ascending=True)\n",
    "                df2 = df_sorted[[\"CHROM\", \"POS\", \"SEGMENT\"]]\n",
    "                # Convert DataFrame to BedTool object\n",
    "                bed_df = pybedtools.BedTool.from_dataframe(df2)\n",
    "                # Group by segment number and find min/max position\n",
    "                result = bed_df.groupby(g=3, c=[2], o=['min', 'max'])\n",
    "                # Convert result to DataFrame\n",
    "                result_df = result.to_dataframe(names=['SEGMENT', 'START', 'END'])\n",
    "                # Group by the 'SEGMENT' column and find min/max values for other columns\n",
    "                result_df = result_df.groupby('SEGMENT').agg({'START': 'min', 'END': 'max'}).reset_index()\n",
    "                # Merge df1 and df2 based on the shared column\n",
    "                merged_df = pd.merge(df_sorted, result_df, on='SEGMENT', how='left')\n",
    "                df1 = merged_df.drop_duplicates(subset=['CHROM', 'START', 'END'])\n",
    "                # Create a Pybedtools object from the BED content\n",
    "                df5 = df1[['CHROM', 'START', 'END']]\n",
    "                df2 = pybedtools.BedTool.from_dataframe(df5)\n",
    "                df2 = df2.merge()\n",
    "                df2 = df2.to_dataframe(names=['CHROM', 'START', 'END'])\n",
    "                f = sum_seg(df2)\n",
    "                f2 = sum_seg2(df2)\n",
    "                seg_len += f\n",
    "                seg_p.extend(f2)\n",
    "                # seg_p.append(f)\n",
    "    if pop  == \"Papuans\":  \n",
    "        adjusted_lens[pop] = int(seg_len)\n",
    "    else:\n",
    "        median = statistics.median(seg_p)\n",
    "        adjusted_lens[pop] = int(median)\n",
    "    segment_lens[pop] = seg_len\n",
    "            \n",
    "df = pd.DataFrame.from_dict(adjusted_lens, orient='index', columns=['Genome covered by detected segments in bases'])\n",
    "df[\"Proportion with covered by detected segment (%)\"] = df['Genome covered by detected segments in bases']/total_len \n",
    "df = df.sort_values(by=\"Genome covered by detected segments in bases\", ascending=False)\n",
    "df \n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6592c-7b49-4218-b983-ec337332d563",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_on(df1, df2):\n",
    "    df1['START'] = None\n",
    "    df1['END'] = None\n",
    "    # print(df1)\n",
    "    # print(\"dfldjfkj\")\n",
    "    # Iterate over each row in df1\n",
    "    start_end_list = list(zip(df2['START'], df2['END']))\n",
    "    match_found = False\n",
    "\n",
    "    for index1, row1 in df1.iterrows():\n",
    "        # Iterate over each row in df2\n",
    "        for start, end in start_end_list:\n",
    "            # Check if POS falls between START and END\n",
    "            if start <= row1['POS'] <= end:\n",
    "                # Set START and END values from df2 to df1\n",
    "                df1.at[index1, 'START'] = start\n",
    "                df1.at[index1, 'END'] = end\n",
    "                break  # Break the inner loop if condition is satisfied\n",
    "# def add_on2(df1,df2):\n",
    "#     df1['START'] = None\n",
    "#     df1['END'] = None\n",
    "#     # print(df1)\n",
    "#     # print(\"dfldjfkj\")\n",
    "#     # Iterate over each row in df1\n",
    "#     start_end_list = list(zip(df2['START'], df2['END']))\n",
    "#     match_found = False\n",
    "#     indices_to_delete = []\n",
    "#     for index1, row1 in df1.iterrows():\n",
    "#         # Iterate over each row in df2\n",
    "#         for start, end in start_end_list:\n",
    "#             # Check if POS falls between START and END\n",
    "#             if row2['START'] <= row1['POS'] <= row2['END']:\n",
    "#                 # Set START and END values from df2 to df1\n",
    "#                 df1.at[index1, 'START'] = row2['START']\n",
    "#                 df1.at[index1, 'END'] = row2['END']\n",
    "#                 match_found = True\n",
    "#                 break  # Break the inner loop if condition is satisfied\n",
    "#         if not match_found:\n",
    "#             indices_to_delete.append(index1)\n",
    "\n",
    "#     df1 = df1.drop(indices_to_delete)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b238b36f-6d32-4c1e-95de-b50036d49e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_chrom_dict = {}\n",
    "for i in range(1,23):\n",
    "    pop_chrom_dict[i] = {}\n",
    "    for pop in populations:\n",
    "        for file in os.listdir(sprime_files):\n",
    "            if pop in file and f\"chr{i}.\" in file:\n",
    "                # print(pop)\n",
    "                df= pd.read_csv(sprime_files + file, sep=\"\\t\")\n",
    "                df[\"CHROM\"] = df[\"CHROM\"].astype(int)\n",
    "                df[\"POS\"] = df[\"POS\"].astype(int)\n",
    "                df[\"SEGMENT\"] = df[\"SEGMENT\"].astype(int)\n",
    "                df = df.loc[df[\"SCORE\"] >= 150000]\n",
    "                df = df.loc[(df[\"NMATCH\"] != \"notcomp\") | (df[\"DMATCH\"] != \"notcomp\")]\n",
    "                df_sorted = df.sort_values(by=[\"CHROM\", \"POS\", \"SEGMENT\"], ascending=True)\n",
    "                df2 = df_sorted[[\"CHROM\", \"POS\", \"SEGMENT\"]]\n",
    "                # Convert DataFrame to BedTool object\n",
    "                bed_df = pybedtools.BedTool.from_dataframe(df2)\n",
    "                # Group by segment number and find min/max position\n",
    "                result = bed_df.groupby(g=3, c=[2], o=['min', 'max'])\n",
    "                # Convert result to DataFrame\n",
    "                result_df = result.to_dataframe(names=['SEGMENT', 'START', 'END'])\n",
    "                # Group by the 'SEGMENT' column and find min/max values for other columns\n",
    "                result_df = result_df.groupby('SEGMENT').agg({'START': 'min', 'END': 'max'}).reset_index()\n",
    "                # Merge df1 and df2 based on the shared column\n",
    "                merged_df = pd.merge(df_sorted, result_df, on='SEGMENT', how='left')\n",
    "                df1 = merged_df.drop_duplicates(subset=['CHROM', 'START', 'END'])\n",
    "                # print(df1)\n",
    "                # print(\"hellodf\")\n",
    "                # Create a Pybedtools object from the BED content\n",
    "                df5 = df1[['CHROM', 'START', 'END']]\n",
    "                # print(df5)\n",
    "                df2 = pybedtools.BedTool.from_dataframe(df5)\n",
    "                df2 = df2.merge()\n",
    "                df2 = df2.to_dataframe(names=['CHROM', 'START', 'END'])\n",
    "                # print(df2)\n",
    "                # print(pop)\n",
    "                add_on(merged_df, df2)\n",
    "                # print(df2)\n",
    "                # print(pop)\n",
    "                # print(lodlkfj)\n",
    "                pop_chrom_dict[i][pop] = merged_df\n",
    "                # print(i)\n",
    "                # print(pop)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d214652-782e-4f2b-bc94-24ae501ec658",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_chrom_dict[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e30987-398f-4be9-a0f3-186ec5b30b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop_chrom_dict[1]\n",
    "pop_chrom_dict[1][\"Papuans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8940e-057e-4093-96b7-763009a1f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k =pop_chrom_dict[1][\"Papuans\"]\n",
    "# k = k.loc[k[\"DMATCH\"] == \"notcomp\"]\n",
    "# k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3f2bb-bb7f-4967-bc79-dface75919f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dict = {}\n",
    "\n",
    "# Iterate over each unique name\n",
    "for name in set(name for chrom_dict in pop_chrom_dict.values() for name in chrom_dict.keys()):\n",
    "    # Collect DataFrames associated with the current name across chromosomes\n",
    "    dfs = [chrom_dict[name] for chrom_dict in pop_chrom_dict.values() if name in chrom_dict]\n",
    "    \n",
    "    # Concatenate DataFrames into a single DataFrame\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    concatenated_df = concatenated_df.sort_values(by=[\"CHROM\", \"POS\"], ascending=True)\n",
    "    \n",
    "    # Store the concatenated DataFrame in the new dictionary\n",
    "    concatenated_dict[name] = concatenated_df\n",
    "concatenated_dict[\"CLM\"]\n",
    "# concatenated_dict[\"Papuans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275f6c8-4125-45da-88de-bc26a9c2fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = concatenated_dict[\"PUR\"]\n",
    "print(k)\n",
    "# print(k.SEGMENT.unique())\n",
    "# print(sorted(k.START.unique()))\n",
    "# print(len(set(sorted(k.END.unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe62042-967a-4df8-b844-41ecd254be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_on2(df1,df2):\n",
    "    df1['START'] = None\n",
    "    df1['END'] = None\n",
    "    # print(df1)\n",
    "    # print(\"dfldjfkj\")\n",
    "    # Iterate over each row in df1\n",
    "    start_end_list = list(zip(df2['START'], df2['END']))\n",
    "    match_found = False\n",
    "    indices_to_delete = []\n",
    "    for index1, row1 in df1.iterrows():\n",
    "        # Iterate over each row in df2\n",
    "        for start, end in start_end_list:\n",
    "            # Check if POS falls between START and END\n",
    "            if start <= row1['POS'] <= end:\n",
    "                # Set START and END values from df2 to df1\n",
    "                df1.at[index1, 'START'] = start\n",
    "                df1.at[index1, 'END'] = end\n",
    "                match_found = True\n",
    "                break  # Break the inner loop if condition is satisfied\n",
    "        if not match_found:\n",
    "            indices_to_delete.append(index1)\n",
    "\n",
    "    df1 = df1.drop(indices_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80779431-082d-41fb-9502-9ebd91469bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_dict = {}\n",
    "population_pairs_tuples = [tuple(pair) for pair in population_pairs]\n",
    "\n",
    "# Initialize the dictionary using population_pairs_tuples\n",
    "ov_dict_more_info = {k: {} for k in population_pairs_tuples}\n",
    "# print(ov_dict_more_info)\n",
    "# ov_dict_more_info = {k: {} for k in population_pairs}\n",
    "# Iterate over chromosomes\n",
    "lst = [\"Papuans\", \"PEL\", \"MXL\", \"CLM\"]\n",
    "\n",
    "for pop in populations:\n",
    "    count = 0\n",
    "    # print(pop)\n",
    "    k = [x for x in population_pairs if pop in x]\n",
    "    # print(k)\n",
    "    df11 = concatenated_dict[pop]\n",
    "    df1 = df11.sort_values(by=[\"CHROM\", \"START\"], ascending=True)\n",
    "    # print(df1)\n",
    "    df1 = df1.drop_duplicates(subset=['CHROM', 'START', 'END'])\n",
    "    df1 = df1[[\"CHROM\", \"START\", \"END\"]]\n",
    "    # print(df1)\n",
    "    df1 = pybedtools.BedTool.from_dataframe(df1)\n",
    "    # print(pop)\n",
    "    for q in k:\n",
    "        # print(q)\n",
    "        # print(dlfjkdljf)\n",
    "        ov_name = \"-\".join(q)\n",
    "        if ov_name not in ov_dict.keys():\n",
    "            pair = q.copy()\n",
    "            # print(pair)\n",
    "            pair.remove(pop)\n",
    "            # print(pair)\n",
    "            df22 = concatenated_dict[pair[0]]\n",
    "            df2 =df22.drop_duplicates(subset=['CHROM', 'START', 'END'])\n",
    "            df2 = df2.sort_values(by=[\"CHROM\", \"START\"], ascending=True)\n",
    "            # print(df2)\n",
    "            df2 = df2[[\"CHROM\", \"START\", \"END\"]]\n",
    "            df2 = pybedtools.BedTool.from_dataframe(df2)\n",
    "            intersected_bed= df1.intersect(df2).sort().merge()\n",
    "            ov_df = intersected_bed.to_dataframe(names=['CHROM', 'START', 'END'])#intersected_df\n",
    "            # print(df11)\n",
    "            # add_on2(df11, ov_df)\n",
    "            # add_on2(df22, ov_df)\n",
    "            # print(df1)\n",
    "            # print(dlfjldfjk)\n",
    "            # print(ov_df)\n",
    "            # print(dlfkjdlfj)\n",
    "            ov_dict[ov_name] = ov_df.sort_values(by=[\"CHROM\", \"START\"], ascending=True)\n",
    "            # print(df11)\n",
    "            if pop in lst and pair[0] in lst:\n",
    "                ov_dict_more_info[tuple(q)][pop] = df11\n",
    "                # print(df11)\n",
    "                # print(ov_dict_more_info[ov_name][pop] )\n",
    "                # print(lol)\n",
    "                ov_dict_more_info[tuple(q)][pair[0]] = df22\n",
    "                ov_dict_more_info[tuple(q)][ov_name] = ov_df\n",
    "            count += 1\n",
    "            del df2 \n",
    "ov_dict['CHB-ITU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5790f8-2dba-4dc7-935e-6185a833b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_dict_more_info.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc8fcb-9e3b-42db-b684-8c66a4043e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_dict_more_info[(\"CLM\", \"Papuans\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6ede7-791f-42d8-a89e-a1f671f8de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_on2(df1,df2):\n",
    "    df1['START'] = None\n",
    "    df1['END'] = None\n",
    "    # print(df1)\n",
    "    # print(\"dfldjfkj\")\n",
    "    # Iterate over each row in df1\n",
    "    start_end_list = list(zip(df2['START'], df2['END']))\n",
    "    match_found = False\n",
    "    indices_to_delete = []\n",
    "    for index1, row1 in df1.iterrows():\n",
    "        # Iterate over each row in df2\n",
    "        for start, end in start_end_list:\n",
    "            # Check if POS falls between START and END\n",
    "            if start <= row1['POS'] <= end:\n",
    "                # Set START and END values from df2 to df1\n",
    "                df1.at[index1, 'START'] = start\n",
    "                df1.at[index1, 'END'] = end\n",
    "                match_found = True\n",
    "                break  # Break the inner loop if condition is satisfied\n",
    "\n",
    "    df1 = df1.dropna()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a69072-082f-40c9-8e92-e3eae17c92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ov_dict_more_info[(\"CLM\", \"Papuans\")][\"CLM\"]\n",
    "df2 = ov_dict_more_info[(\"CLM\", \"Papuans\")][\"Papuans\"]\n",
    "ov = ov_dict_more_info[(\"CLM\", \"Papuans\")][\"CLM-Papuans\"]\n",
    "add_on2(df1, ov)\n",
    "add_on2(df2, ov)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d28ff6-5056-4752-82c1-9f145c9852df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna()  \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3e580-8db8-486f-9331-5ce69a706026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.dropna()    \n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd5763-0126-4c0a-a591-8380d30fdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_p2 = [tuple(pair) for pair in population_pairs if pair[0] in lst and pair[1] in lst]\n",
    "population_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4885b2-6118-4c64-82b8-15d0cb0d3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addy(k):\n",
    "    df1 = ov_dict_more_info[k][k[0]]\n",
    "    df2 = ov_dict_more_info[k][k[1]]\n",
    "    ov = ov_dict_more_info[k]['-'.join(k)]\n",
    "    add_on2(df1, ov)\n",
    "    add_on2(df2, ov)\n",
    "    df1 = df1.dropna()  \n",
    "    df2 = df2.dropna()  \n",
    "    concat_df = pd.concat([df1, df2])\n",
    "    dedup_df = concat_df.drop_duplicates()\n",
    "    return dedup_df\n",
    "\n",
    "concati = {}\n",
    "for p in population_p2:\n",
    "    r = addy(p)\n",
    "    concati[p] = r  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41cdf0-c4a8-468a-bf85-083a584d096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(h):\n",
    "    z = concati[h]\n",
    "    nmatch = (z['NMATCH'] == 'match').sum()\n",
    "    dmatch = (z['DMATCH'] == 'match').sum()\n",
    "    nmis = (z['NMATCH'] == 'mismatch').sum()\n",
    "    dmis = (z['DMATCH'] == 'mismatch').sum()\n",
    "    n= nmatch/(nmatch + nmis)\n",
    "    d= dmatch/(dmatch + dmis)\n",
    "    return n, d\n",
    "\n",
    "def calc2(h):\n",
    "    m = concati[h]\n",
    "    Nean = []\n",
    "    Deni = []\n",
    "    for chrom in m.CHROM.unique():\n",
    "        n = m.loc[m[\"CHROM\"] == chrom] \n",
    "        q = n[\"START\"].unique()\n",
    "        for seg in q:\n",
    "            z = n.loc[n[\"START\"] == seg]\n",
    "            nmatch = (z['NMATCH'] == 'match').sum()\n",
    "            dmatch = (z['DMATCH'] == 'match').sum()\n",
    "            nmis = (z['NMATCH'] == 'mismatch').sum()\n",
    "            dmis = (z['DMATCH'] == 'mismatch').sum()\n",
    "            Nean.append(nmatch/(nmatch + nmis))\n",
    "            Deni.append(dmatch/(dmatch + dmis))\n",
    "    return Nean, Deni\n",
    "\n",
    "        \n",
    "nean = [calc(x)[0] for x in population_p2]\n",
    "deni = [calc(x)[1] for x in population_p2]\n",
    "# nean    \n",
    "\n",
    "nean_d = {x:calc2(x)[0] for x in population_p2}\n",
    "deni_d = {x: calc2(x)[1] for x in population_p2}\n",
    "# nean_d\n",
    "population_p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec0873-6c36-4c76-bdaf-0a6580969892",
   "metadata": {},
   "outputs": [],
   "source": [
    "nean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81f37f-e065-4725-ba25-5d6d78168327",
   "metadata": {},
   "outputs": [],
   "source": [
    "deni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c700c-f8c9-464e-b6eb-d543012b40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_list = [0.52, 0.54, 0.56, 0.5800000000000001, 0.6, 0.62, 0.64, 0.66, 0.68, 0.7, 0.72]\n",
    "\n",
    "# doubled_list = []\n",
    "# for i in range(len(original_list) - 1):\n",
    "#     doubled_list.append(original_list[i])\n",
    "#     doubled_list.append((original_list[i] + original_list[i+1]) / 2)\n",
    "\n",
    "# doubled_list.append(original_list[-1])\n",
    "\n",
    "# print(doubled_list)\n",
    "original_list = [0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29000000000000004, 0.3, 0.31, 0.32]\n",
    "\n",
    "\n",
    "doubled_list = [original_list[0]]\n",
    "for i in range(len(original_list) - 1):\n",
    "    # Calculate the half step between current and next elements\n",
    "    half_step = (original_list[i+1] - original_list[i]) / 2\n",
    "    # Add the current element and the half step\n",
    "    doubled_list.append(original_list[i] + half_step)\n",
    "    doubled_list.append(original_list[i+1])\n",
    "\n",
    "print(doubled_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a9d03-9477-40eb-bac0-5737741d6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data\n",
    "names = population_p2\n",
    "print(names)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(nean, deni)\n",
    "\n",
    "# Annotate each point with its corresponding name\n",
    "for name, x, y in zip(names, nean, deni):\n",
    "    plt.annotate(name, (x, y), textcoords=\"offset points\", xytext=(2, 2), ha='center')\n",
    "# Get current tick marks\n",
    "current_xticks = plt.xticks()[0]\n",
    "current_yticks = plt.yticks()[0]\n",
    "# print(current_yticks)\n",
    "t = [0.02*x for x in range(1, 2)]\n",
    "tt = [0.01*x for x in range(1, 2)]\n",
    "# print(t)\n",
    "# print(len(t))\n",
    "y = [x + 0.7 for x in t]\n",
    "yy = [x + 0.31 for x in tt]\n",
    "# print(y)\n",
    "# print(len(y))\n",
    "q = [x for x in current_xticks]\n",
    "qq = [x for x in current_yticks]\n",
    "\n",
    "# print(q)\n",
    "q.extend(y)\n",
    "qq.extend(yy)\n",
    "# print(q)\n",
    "# print(qq)\n",
    "# q = [0.52, 0.53, 0.54, 0.55, 0.56, 0.5700000000000001, 0.5800000000000001, 0.5900000000000001, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72]\n",
    "# qq = [0.22, 0.225, 0.23, 0.235, 0.24, 0.245, 0.25, 0.255, 0.26, 0.265, 0.27, 0.275, 0.28, 0.28500000000000003, 0.29000000000000004, 0.29500000000000004, 0.3, 0.305, 0.31, 0.315, 0.32]\n",
    "\n",
    "# # Set custom tick marks for both x and y axes\n",
    "plt.xticks(q)\n",
    "plt.yticks(qq)\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.title('Neanderthal vs Denisovan Proportions with amongst population pairs of interest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ef439-6e88-42c3-8a03-2875119bce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot KDE plots on each subplot\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    Neanderthal = nean_d[population_p2[i]]\n",
    "    Denisovan = deni_d[population_p2[i]]\n",
    "    # sns.kdeplot(x=Neanderthal, y=Denisovan, linestyles=\":\", n_levels=22, alpha=0.6)\n",
    "    # sns.kdeplot(x=Neanderthal, y=Denisovan, cbar=True, cmap='Blues', n_levels=22, shade=True, thresh=0)\n",
    "    sns.kdeplot(x=Neanderthal, y=Denisovan, cbar=True, cmap='Blues', n_levels=22, fill=True, thresh=0, ax=ax)\n",
    "\n",
    "    # sns.kdeplot(x=Neanderthal, y=Denisovan, linestyles=\":\", n_levels=22, alpha=0.6, cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'{\"-\".join(population_p2[i])} proportion')\n",
    "    ax.set_xlabel('Neanderthal proportion')\n",
    "    ax.set_ylabel('Denisovan proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660eec45-5287-408e-a27c-294809979193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Sample data for neanderthal and denisovan\n",
    "# nean = [0.1, 0.3, 0.4, 0.5, 0.6]\n",
    "# deni = [0.2, 0.3, 0.4, 0.5, 0.7]\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.set_style(\"white\")\n",
    "# sns.set(rc={\"lines.linewidth\": 0.2})\n",
    "# plt.ylim(0, 1)\n",
    "# plt.xlim(0, 1)\n",
    "# plt.ylabel(\"Denisovan\", fontsize=14)\n",
    "# plt.xlabel(\"Neanderthal\", fontsize=14)\n",
    "# plt.title(\"Neanderthal Denisovan Introgressed \\nRegions for overlaps\", fontsize=16, weight='bold')\n",
    "\n",
    "# sns.kdeplot(x=nean, y=deni, linestyles=\":\", n_levels=22, alpha=0.6)\n",
    "# sns.kdeplot(x=nean, y=deni, cbar=True, cmap='Blues', n_levels=3, shade=True, thresh=0, cbar_kws={'label': 'Density', 'orientation': 'vertical'})\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713924d-2b03-4980-8bdf-d48fc8057a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data for neanderthal and denisovan\n",
    "nean = [0.1, 0.3, 0.4, 0.5, 0.6]\n",
    "deni = [0.2, 0.3, 0.4, 0.5, 0.7]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.set_style(\"white\")\n",
    "sns.set(rc={\"lines.linewidth\": 0.2})\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylabel(\"Denisovan\", fontsize=14)\n",
    "plt.xlabel(\"Neanderthal\", fontsize=14)\n",
    "plt.title(\"Neanderthal Denisovan Introgressed \\nRegions for overlaps\", fontsize=16, weight='bold')\n",
    "\n",
    "# Create the KDE plot with vmin and vmax set to 0 and 1 respectively\n",
    "kde = sns.kdeplot(x=nean, y=deni, linestyles=\":\",  alpha=0.6, cmap='Blues')\n",
    "\n",
    "# Add color bar\n",
    "# plt.colorbar(kde.collections[0], label='Density')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118b617-ab89-44b8-87e9-45f37f759e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_chrom_df = pd.DataFrame(columns=populations, index=populations)\n",
    "ov_chrom_df_overlaps = pd.DataFrame(columns=populations, index=populations)\n",
    "pop_set = set()\n",
    "for pop in populations:\n",
    "    ov_chrom_df.at[pop,pop] = 1\n",
    "    k = [list(x) for x in population_pairs if pop in x]\n",
    "    sum_pop = segment_lens[pop]\n",
    "    for ele in k:\n",
    "        ov_name = \"-\".join(ele)\n",
    "        pair = ele.copy()\n",
    "        pair.remove(pop)\n",
    "        df_both = ov_dict[ov_name]\n",
    "        df_both = df_both.drop_duplicates(subset=['CHROM', 'START', 'END'])\n",
    "        sum_b = sum_seg(df_both)\n",
    "        ov_chrom_df.at[pop, pair[0]] = sum_b/sum_pop\n",
    "        ov_chrom_df_overlaps[pop, pair[0]] = sum_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23822821-dca7-4b03-927d-d04843d8aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ov_chrom_df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb408e0-2bee-4a12-a34b-ac8249d9b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {tuple(([row, col])): df.at[row, col] for row in df.index for col in df.columns}\n",
    "# result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09697bd2-3489-4ada-abb5-7cdb16cdd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = [key for key, value in result_dict.items() if value == 1]\n",
    "for key in keys_to_remove:\n",
    "    del result_dict[key]\n",
    "import matplotlib.pyplot as plt\n",
    "x_values = []\n",
    "y_values = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "lst = [\"Papuans\", \"PEL\", \"MXL\", \"CLM\"]\n",
    "for key, value in result_dict.items():\n",
    "    if key[0] in lst and key[1] in lst:\n",
    "        x_values.append(value)\n",
    "        y_values.append(value)\n",
    "        labels.append(f\"{key[0][:3]}-{key[1][:3]}\")\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(16, 12))  # Adjust figure size as needed\n",
    "# plt.scatter(x_values, y_values)\n",
    "# lst = \n",
    "\n",
    "# Add labels to data points\n",
    "for labele, x, y in zip(labels, x_values, y_values):\n",
    "    \n",
    "    # plt.scatter(x, y, label=labele) #has legend\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(labele, (x, y), textcoords=\"offset points\", xytext=(10, 10), ha='center')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add labels to axes\n",
    "plt.xlabel('Sharing proportion')\n",
    "plt.ylabel('Sharing proportion')\n",
    "current_xticks = plt.xticks()[0]\n",
    "# lst = [0.00025*x for x in range(12)]\n",
    "# lst = [0.000125*x for x in range(24)]\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.title('Plot showcasing pairwise sharing proportion in regards to putatively introgressed segments')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8d662-8d73-49a5-8a1b-b2f983c1b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = [key for key, value in result_dict.items() if value == 1]\n",
    "for key in keys_to_remove:\n",
    "    del result_dict[key]\n",
    "import matplotlib.pyplot as plt\n",
    "x_values = []\n",
    "# print(x_values)\n",
    "# print(dlfhd)\n",
    "y_values = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "lst = [\"Papuans\", \"PEL\", \"MXL\", \"CLM\"]\n",
    "for key, value in result_dict.items():\n",
    "        if key[0] in lst and key[1] in lst:\n",
    "            x_values.append(key[0])\n",
    "            y_values.append(value)\n",
    "            labels.append(f\"{key[0][:3]}-{key[1][:3]}\")\n",
    "    \n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(16, 12))  # Adjust figure size as needed\n",
    "# # Add labels to data points\n",
    "for labele, x, y in zip(labels, x_values, y_values):\n",
    "    \n",
    "    # plt.scatter(x, y, label=labele) #has legend\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(labele, (x, y), textcoords=\"offset points\", xytext=(10, 10), ha='center')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add labels to axes\n",
    "plt.xlabel('Populations')\n",
    "plt.ylabel('Sharing proportion')\n",
    "current_yticks = plt.yticks()[0]\n",
    "\n",
    "# Find the minimum and maximum values of the original list\n",
    "min_value = np.min(y_values) + 0.0005\n",
    "max_value = np.max(y_values)+0.0005\n",
    "\n",
    "# Generate a new list with twice the number of elements\n",
    "new_list = np.linspace(min_value, max_value, len(current_yticks) * 2)\n",
    "# lst = [0.00025*x for x in range(12)]\n",
    "# lst = [0.000125*x for x in range(24)]\n",
    "plt.xticks()\n",
    "plt.yticks(new_list)\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.title('Plot showcasing pairwise sharing proportion in regards to putatively introgressed segments')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cb033-c3a0-4784-b159-c68485028542",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = [key for key, value in result_dict.items() if value == 1]\n",
    "for key in keys_to_remove:\n",
    "    del result_dict[key]\n",
    "import matplotlib.pyplot as plt\n",
    "x_values = []\n",
    "y_values = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "lst = [\"Papuans\", \"PEL\", \"MXL\", \"CLM\"]\n",
    "for key, value in result_dict.items():\n",
    "    if key[0] in lst or key[1] in lst:\n",
    "        x_values.append(value)\n",
    "        y_values.append(value)\n",
    "        labels.append(f\"{key[0][:3]}-{key[1][:3]}\")\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(16, 12))  # Adjust figure size as needed\n",
    "# plt.scatter(x_values, y_values)\n",
    "# lst = \n",
    "\n",
    "# Add labels to data points\n",
    "for labele, x, y in zip(labels, x_values, y_values):\n",
    "    \n",
    "    # plt.scatter(x, y, label=labele) #has legend\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(labele, (x, y), textcoords=\"offset points\", xytext=(10, 10), ha='center')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add labels to axes\n",
    "plt.xlabel('Sharing proportion')\n",
    "plt.ylabel('Sharing proportion')\n",
    "current_xticks = plt.xticks()[0]\n",
    "print(current_xticks)\n",
    "# new_xticks = [tick * 2 for tick in current_xticks]\n",
    "# Find the minimum and maximum values of the original list\n",
    "min_value = np.min(x_values)\n",
    "max_value = np.max(current_xticks)\n",
    "new_xticks = np.linspace(min_value, max_value, len(original_list) * 2)\n",
    "new_xtics = np.array([round(num, 2) for num in new_xticks])\n",
    "\n",
    "print(new_xticks)\n",
    "# print(dlfjdlj)\n",
    "# lst = [0.00025*x for x in range(12)]\n",
    "# lst = [0.000125*x for x in range(24)]\n",
    "# Determine the range and step size for the x-axis tick marks\n",
    "\n",
    "# Generate the tick marks\n",
    "# x_ticks = [x_min + i * x_step for i in range(int((x_max - x_min) / x_step) + 1)]\n",
    "# print(x_ticks)\n",
    "plt.xticks(ticks=new_xticks)\n",
    "plt.yticks()\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.title('Plot showcasing pairwise sharing proportion in regards to putatively introgressed segments')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ce4e0-634d-42cb-812c-3749963c2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = [key for key, value in result_dict.items() if value == 1]\n",
    "for key in keys_to_remove:\n",
    "    del result_dict[key]\n",
    "import matplotlib.pyplot as plt\n",
    "x_values = []\n",
    "# print(x_values)\n",
    "# print(dlfhd)\n",
    "y_values = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "lst = [\"Papuans\", \"PEL\", \"MXL\", \"CLM\"]\n",
    "for key, value in result_dict.items():\n",
    "        x_values.append(key[0])\n",
    "        y_values.append(value)\n",
    "        labels.append(f\"{key[0][:3]}-{key[1][:3]}\")\n",
    "    \n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(16, 12))  # Adjust figure size as needed\n",
    "# # Add labels to data points\n",
    "for labele, x, y in zip(labels, x_values, y_values):\n",
    "    \n",
    "    # plt.scatter(x, y, label=labele) #has legend\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(labele, (x, y), textcoords=\"offset points\", xytext=(10, 10), ha='center')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add labels to axes\n",
    "plt.xlabel('Populations')\n",
    "plt.ylabel('Sharing proportion')\n",
    "current_xticks = plt.xticks()[0]\n",
    "\n",
    "# Find the minimum and maximum values of the original list\n",
    "min_value = np.min(current_xticks)\n",
    "max_value = np.max(current_xticks)\n",
    "\n",
    "# Generate a new list with twice the number of elements\n",
    "new_list = np.linspace(min_value, max_value, len(original_list) * 2)\n",
    "# lst = [0.00025*x for x in range(12)]\n",
    "# lst = [0.000125*x for x in range(24)]\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.title('Plot showcasing pairwise sharing proportion in regards to putatively introgressed segments')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c99034-17ad-457e-abad-812021d589e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = [key for key, value in result_dict.items() if value == 1]\n",
    "for key in keys_to_remove:\n",
    "    del result_dict[key]\n",
    "import matplotlib.pyplot as plt\n",
    "x_values = []\n",
    "# print(x_values)\n",
    "# print(dlfhd)\n",
    "y_values = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "lst = [\"Papuans\", \"PEL\", \"MXL\", \"CLM\"]\n",
    "for key, value in result_dict.items():\n",
    "        if key[0] in lst and key[1] in lst:\n",
    "            x_values.append(key[0])\n",
    "            y_values.append(value)\n",
    "            labels.append(f\"{key[0][:3]}-{key[1][:3]}\")\n",
    "    \n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(16, 12))  # Adjust figure size as needed\n",
    "# # Add labels to data points\n",
    "for labele, x, y in zip(labels, x_values, y_values):\n",
    "    \n",
    "    # plt.scatter(x, y, label=labele) #has legend\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(labele, (x, y), textcoords=\"offset points\", xytext=(10, 10), ha='center')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add labels to axes\n",
    "plt.xlabel('Populations')\n",
    "plt.ylabel('Sharing proportion')\n",
    "current_yticks = plt.yticks()[0]\n",
    "\n",
    "# Find the minimum and maximum values of the original list\n",
    "min_value = np.min(y_values) + 0.0005\n",
    "max_value = np.max(y_values)+0.0005\n",
    "\n",
    "# Generate a new list with twice the number of elements\n",
    "new_list = np.linspace(min_value, max_value, len(current_yticks) * 2)\n",
    "# lst = [0.00025*x for x in range(12)]\n",
    "# lst = [0.000125*x for x in range(24)]\n",
    "plt.xticks()\n",
    "plt.yticks(new_list)\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.title('Plot showcasing pairwise sharing proportion in regards to putatively introgressed segments')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
